游붗 Fine-Tuning LLM estilo Alpaca con Hugging Face AutoTrain

Este repositorio contiene un script en formato Jupyter Notebook para realizar el fine-tuning de un modelo de lenguaje grande (LLM) utilizando el estilo de entrenamiento de Alpaca mediante la plataforma Hugging Face AutoTrain. Es parte de una propuesta t칠cnica presentada para una posici칩n en el 치rea de inteligencia artificial.

游늯 Archivo principal
11_1_autotrain_alpaca.ipynb: Notebook interactivo para configurar y lanzar el fine-tuning en AutoTrain.
11_1_autotrain_alpaca.py: Versi칩n del notebook convertida a script Python ejecutable.

Caracter칤sticas principales
Utiliza un dataset estructurado con campos instruction, input y output.

  Preprocesamiento adaptado para formato text2text compatible con AutoTrain.
  Entrenamiento supervisado con configuraci칩n m칤nima y reproducible.
  Integraci칩n directa con Hugging Face Hub (sin necesidad de infraestructura local).
  Pensado para experimentaci칩n r치pida con LLMs de tipo generativo.

Modelo objetivo
Aunque el c칩digo permite trabajar con distintos modelos, est치 optimizado para modelos tipo gpt2, mistral o modelos open-source compatibles con fine-tuning estilo Instruct.

游늭 Estructura esperada del dataset
El dataset debe tener el siguiente formato por registro JSON o CSV:

json
Copiar
Editar
{
  "instruction": "Describe the benefits of aquaculture AI.",
  "input": "",
  "output": "AI in aquaculture allows better feed optimization and water quality monitoring."
}
游 쮺칩mo usar?
Aseg칰rate de tener acceso a Hugging Face AutoTrain y haber creado un token de acceso.

Prepara tu dataset en el formato Alpaca.

Ejecuta el notebook paso a paso, autenticando con tu token y cargando el dataset.

Configura los par치metros b치sicos del proyecto (nombre, tipo, modelo base).

Lanza el entrenamiento desde el entorno AutoTrain.

Requisitos
Cuenta de Hugging Face.
Token de acceso v치lido con permisos de escritura.
Dataset preformateado (JSON o CSV).
Entorno Python 3.8+ con acceso a Jupyter Notebook.

Aplicaci칩n y contexto
Este script se incluye como parte de una postulaci칩n para posiciones relacionadas con desarrollo de IA, fine-tuning de LLMs y NLP aplicado a dominios t칠cnicos y cient칤ficos, especialmente en contextos donde se requiere eficiencia, escalabilidad y adaptabilidad en la creaci칩n de asistentes conversacionales o sistemas instructivos.

 Contacto
Para cualquier consulta t칠cnica o colaboraci칩n:

Bruno Alejandro Donayre Donayre
Correo: brunodonayredonadyre@gmail.com


