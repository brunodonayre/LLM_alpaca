 Fine-Tuning LLM estilo Alpaca con Hugging Face AutoTrain

Este repositorio contiene un script en formato Jupyter Notebook para realizar el fine-tuning de un modelo de lenguaje grande (LLM) utilizando el estilo de entrenamiento de Alpaca mediante la plataforma Hugging Face AutoTrain. Es parte de una propuesta t茅cnica presentada para una posici贸n en el 谩rea de inteligencia artificial.

 Archivo principal
11_1_autotrain_alpaca.ipynb: Notebook que automatiza el preprocesamiento, configuraci贸n y lanzamiento del entrenamiento supervisado de un modelo tipo Instruct (estilo Alpaca) a trav茅s de AutoTrain.

锔 Caracter铆sticas principales
Utiliza un dataset estructurado con campos instruction, input y output.

Preprocesamiento adaptado para formato text2text compatible con AutoTrain.

Entrenamiento supervisado con configuraci贸n m铆nima y reproducible.

Integraci贸n directa con Hugging Face Hub (sin necesidad de infraestructura local).

Pensado para experimentaci贸n r谩pida con LLMs de tipo generativo.

 Modelo objetivo
Aunque el c贸digo permite trabajar con distintos modelos, est谩 optimizado para modelos tipo gpt2, mistral o modelos open-source compatibles con fine-tuning estilo Instruct.

 Estructura esperada del dataset
El dataset debe tener el siguiente formato por registro JSON o CSV:

json
Copiar
Editar
{
  "instruction": "Describe the benefits of aquaculture AI.",
  "input": "",
  "output": "AI in aquaculture allows better feed optimization and water quality monitoring."
}
 驴C贸mo usar?
Aseg煤rate de tener acceso a Hugging Face AutoTrain y haber creado un token de acceso.

Prepara tu dataset en el formato Alpaca.

Ejecuta el notebook paso a paso, autenticando con tu token y cargando el dataset.

Configura los par谩metros b谩sicos del proyecto (nombre, tipo, modelo base).

Lanza el entrenamiento desde el entorno AutoTrain.

 Requisitos
Cuenta de Hugging Face.

Token de acceso v谩lido con permisos de escritura.

Dataset preformateado (JSON o CSV).

Entorno Python 3.8+ con acceso a Jupyter Notebook.

 Aplicaci贸n y contexto
Este script se incluye como parte de una postulaci贸n para posiciones relacionadas con desarrollo de IA, fine-tuning de LLMs y NLP aplicado a dominios t茅cnicos y cient铆ficos, especialmente en contextos donde se requiere eficiencia, escalabilidad y adaptabilidad en la creaci贸n de asistentes conversacionales o sistemas instructivos.

 Contacto
Para cualquier consulta t茅cnica o colaboraci贸n:

Bruno Alejandro Donayre Donayre
Correo: brunodonayredonadyre@gmail.com
LinkedIn (opcional)

